{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ErnCL4ZMZU-"
      },
      "source": [
        "#### Library Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCEAa7PNMYjc"
      },
      "outputs": [],
      "source": [
        "!pip install skorch torch scikit-learn\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    ConfusionMatrixDisplay,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    precision_recall_curve,\n",
        "    accuracy_score,\n",
        "    log_loss,\n",
        "    PrecisionRecallDisplay,\n",
        "    make_scorer,\n",
        "    RocCurveDisplay\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sns\n",
        "import imblearn\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from xgboost import cv\n",
        "from xgboost import XGBClassifier\n",
        "import scipy as stats\n",
        "from skorch.callbacks import EarlyStopping, Checkpoint, EpochScoring\n",
        "from skorch.helper import predefined_split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00gGl21bI0zu"
      },
      "source": [
        "#### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCey7DavMgKM"
      },
      "outputs": [],
      "source": [
        "file_id_1 = '18c5DynpKSiey55WdTBkNE7Iwb7l_HL-k'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_1}', 'data2011.csv', quiet=False)\n",
        "df1 = pd.read_csv('data2011.csv')\n",
        "\n",
        "file_id_2 = '1bJsC9bUmrMHXlKIv82Gkl-Qxldy9D-KQ'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_2}', 'data2102.csv', quiet=False)\n",
        "df2 = pd.read_csv('data2102.csv')\n",
        "\n",
        "file_id_3 = '1BU41bihK6rCTVWmyUFr4gEmYwIclKeMD'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_3}', 'data2105.csv', quiet=False)\n",
        "df3 = pd.read_csv('data2105.csv')\n",
        "\n",
        "file_id_4 = '1VUA3AgnL7ouqCY3vrui7G6qr5RbbJwDQ'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_4}', 'data2108.csv', quiet=False)\n",
        "df4 = pd.read_csv('data2108.csv')\n",
        "\n",
        "file_id_5 = '1GSL8AOlv9fWylFU-HAKbIbOCxuN1b754'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_5}', 'data2111.csv', quiet=False)\n",
        "df5 = pd.read_csv('data2111.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFCNdYAI4CY"
      },
      "source": [
        "#### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT6U2R4IM1ri"
      },
      "outputs": [],
      "source": [
        "## Rename Columns\n",
        "def rename(df):\n",
        "    return df.rename(columns={\n",
        "        'RREL16': 'primary_income',\n",
        "        'RREL13': 'employment_status',\n",
        "        'RREL27': 'loan_purpose',\n",
        "        'RREL25': 'original_term',\n",
        "        'RREL30': 'current_balance',\n",
        "        'RREL29': 'original_balance',\n",
        "        'RREL43': 'current_interest_rate',\n",
        "        'RREL42': 'interest_type',\n",
        "        'RREL69': 'account_status',\n",
        "        'RREL39': 'payment_due',\n",
        "        'RREL67': 'arrears_balance',\n",
        "        'RREL68': 'days_in_arrears',\n",
        "        'RREL71': 'default_amount',\n",
        "        'RREC6': 'collateral_region',\n",
        "        'RREC7': 'occupancy_type',\n",
        "        'RREC9': 'property_type',\n",
        "        'RREC16': 'original_ltv',\n",
        "        'RREC17': 'original_valuation',\n",
        "        'RREC12': 'current_ltv',\n",
        "        'RREC13': 'current_valuation',\n",
        "        'age': 'age',\n",
        "        'PrepaymentFee': 'prepayment_fee',\n",
        "        'PrepaymentHistory': 'prepayment_history',\n",
        "        'RREL30_t_1': 'past_balance',\n",
        "        'RREL39_t_1': 'past_payment_due',\n",
        "        'RREL43_t_1': 'past_interest_rate',\n",
        "        'RREC12_t_1': 'past_ltv',\n",
        "        'RREC13_t_1': 'past_valuation',\n",
        "        'incentive': 'incentive',\n",
        "        'target': 'target'\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7K_c3xeM8e-"
      },
      "outputs": [],
      "source": [
        "## Embed Categorical columns\n",
        "def embed(df):\n",
        "    df['employment_status'] = df['employment_status'].astype('category')\n",
        "    df['loan_purpose'] = df['loan_purpose'].astype('category')\n",
        "    df['collateral_region'] = df['collateral_region'].astype('category')\n",
        "    df['occupancy_type'] = df['occupancy_type'].astype('category')\n",
        "    df['property_type'] = df['property_type'].astype('category')\n",
        "    df['interest_type'] = df['interest_type'].astype('category')\n",
        "    df['account_status'] = df['account_status'].astype('category')\n",
        "    df['prepayment_fee'] = df['prepayment_fee'].astype('category')\n",
        "    df['prepayment_history'] = df['prepayment_history'].astype('category')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BSpwL4iNNEo"
      },
      "outputs": [],
      "source": [
        "# Rename all datasets\n",
        "df1 = rename(df1)\n",
        "df2 = rename(df2)\n",
        "df3 = rename(df3)\n",
        "df4 = rename(df4)\n",
        "df5 = rename(df5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNIQovr2XzpG"
      },
      "outputs": [],
      "source": [
        "# Drop single employment PNNR observation\n",
        "df5 = df5[df5['employment_status'] != 'PNNR']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB-HQU2VyJI0"
      },
      "outputs": [],
      "source": [
        "# Embed all categorical variables\n",
        "df1 = embed(df1)\n",
        "df2 = embed(df2)\n",
        "df3 = embed(df3)\n",
        "df4 = embed(df4)\n",
        "df5 = embed(df5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzMBYKj4NNqb"
      },
      "outputs": [],
      "source": [
        "# Split data from targets\n",
        "X1 = df1.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y1 = df1['target']\n",
        "X2 = df2.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y2 = df2['target']\n",
        "X3 = df3.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y3 = df3['target']\n",
        "X4 = df4.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y4 = df4['target']\n",
        "X5 = df5.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y5 = df5['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0i6-1aeNyV0h"
      },
      "source": [
        "#### SHAP-Reduction Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to maintain SHAP variables\n",
        "def drop(datasets):\n",
        "    columns_to_keep = ['collateral_region', 'primary_income', 'current_interest_rate', 'original_valuation',\n",
        "                       'original_ltv', 'past_interest_rate', 'age', 'payment_due', 'past_ltv', 'current_ltv',\n",
        "                       'original_balance', 'current_balance', 'current_valuation', 'past_valuation', 'past_payment_due',\n",
        "                       'past_balance', 'property_type', 'occupancy_type', 'original_term', 'loan_purpose']\n",
        "    return [df[columns_to_keep] for df in datasets]\n",
        "\n",
        "# Apply and reassign\n",
        "X1_shap, X2_shap, X3_shap, X4_shap, X5_shap = drop([X1, X2, X3, X4, X5])"
      ],
      "metadata": {
        "id": "v0hz5Ce5ynqW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EuXI81xMd0wC"
      },
      "outputs": [],
      "source": [
        "# Create train and test data\n",
        "X_shap_train = pd.concat([X1_shap, X2_shap], ignore_index=True)\n",
        "y_shap_train = pd.concat([y1, y2], ignore_index=True)\n",
        "X_shap_val = pd.concat([X3_shap, X4_shap], ignore_index=True)\n",
        "y_shap_val = pd.concat([y3, y4], ignore_index=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWknZPH6apwL"
      },
      "source": [
        "### SHAP-reduced RF Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'min_child_weight': [1, 5, 10],\n",
        "    'reg_alpha': [0, 0.1, 1],\n",
        "    'reg_lambda': [0, 1, 10],\n",
        "    'scale_pos_weight': [50, 100],\n",
        "    'n_estimators': [500, 1000, 3000]  # Moved inside param_grid\n",
        "}\n",
        "\n",
        "# Set up XGBoost without early stopping in the initializer\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric=['aucpr', 'logloss'],\n",
        "    enable_categorical=True,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 5-Fold Stratified Cross Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Custom scoring for imbalanced data\n",
        "scoring = {\n",
        "    'precision': make_scorer(precision_score, zero_division=0),\n",
        "    'recall': make_scorer(recall_score, zero_division=0),\n",
        "    'f1': make_scorer(f1_score, zero_division=0),\n",
        "    'aucpr': 'average_precision'\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "grid = RandomizedSearchCV(\n",
        "    estimator=xgb_clf,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=30,  # Reduced for faster execution\n",
        "    scoring=scoring,\n",
        "    refit='aucpr',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit without early stopping in grid search\n",
        "grid.fit(X_shap_train, y_shap_train)\n",
        "\n",
        "# Now train final model using best params\n",
        "best_params = grid.best_params_.copy()\n",
        "\n",
        "\n",
        "final_model = xgb.XGBClassifier(\n",
        "    **best_params,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric=['aucpr', 'logloss'],\n",
        "    early_stopping_rounds=50,\n",
        "    enable_categorical=True,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "final_model.fit(\n",
        "    X_shap_train, y_shap_train,\n",
        "    eval_set=[(X_shap_val, y_shap_val)],\n",
        "    verbose=True\n",
        ")\n",
        "# Best model\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate on test set with optimal threshold\n",
        "y_proba = final_model.predict_proba(X_shap_val)[:, 1]\n",
        "\n",
        "# Find optimal threshold\n",
        "precision, recall, thresholds = precision_recall_curve(y_shap_val, y_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "# Metrics\n",
        "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_shap_val, y_pred)}\")\n",
        "print(f\"Precision: {precision_score(y_shap_val, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_shap_val, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_shap_val, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"ROC AUC:   {roc_auc_score(y_shap_val, y_proba):.4f}\")\n",
        "print(f\"PR AUC:    {average_precision_score(y_shap_val, y_proba):.4f}\")\n",
        "\n",
        "# Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = PrecisionRecallDisplay.from_estimator(final_model, X_shap_val, y_shap_val)\n",
        "plt.title('SHAP-Reduced Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.savefig(\"shap_pr.png\", dpi=300, bbox_inches='tight')\n",
        "#files.download(\"shap_pr.png\")\n",
        "plt.show()\n",
        "\n",
        "# ROC Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "RocCurveDisplay.from_estimator(final_model, X_shap_val, y_shap_val)\n",
        "plt.title('SHAP-Reduced ROC Curve', fontsize=12, pad=20)\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "plt.legend(loc='lower right', fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()\n",
        "\n",
        "# Learning curves\n",
        "results = final_model.evals_result()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(results['validation_0']['logloss'], label='Test Log Loss')\n",
        "plt.plot(results['validation_0']['aucpr'], label='Test AUC-PR')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nhHwYmoVzYAm"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1ErnCL4ZMZU-",
        "00gGl21bI0zu",
        "TEFCNdYAI4CY",
        "byT3Z69MI9kt",
        "csSMBzN0yknJ",
        "ax1bgDWTynzk",
        "Jkq-2aulyxOx",
        "RGhfFRGDPIeV",
        "UvdMmMGAyt_S"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}