{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "Y63Hk81eg69j",
      "metadata": {
        "id": "Y63Hk81eg69j"
      },
      "source": [
        "# Phase 1: Test & Train on 1 Window (D2011)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc247b72-fb43-466d-ab4e-32dba1a8c74d",
      "metadata": {
        "id": "cc247b72-fb43-466d-ab4e-32dba1a8c74d"
      },
      "source": [
        "### Library and Data Imports"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Core Libraries for Model Training and Evaluation ---\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "!pip install skorch torch scikit-learn\n",
        "# Scikit-Learn Core Components\n",
        "from sklearn.model_selection import RandomizedSearchCV, StratifiedKFold, learning_curve, train_test_split, GridSearchCV\n",
        "from sklearn.metrics import (make_scorer, precision_score, recall_score, f1_score,\n",
        "                             roc_auc_score, average_precision_score, precision_recall_curve,\n",
        "                             confusion_matrix, PrecisionRecallDisplay, ConfusionMatrixDisplay)\n",
        "# XGBoost Specific\n",
        "import xgboost as xgb\n",
        "import shap  # Only for XGBoost SHAP analysis\n",
        "\n",
        "# Scikit-Learn Models\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# PyTorch & skorch\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from skorch import NeuralNetClassifier\n",
        "from skorch.callbacks import EarlyStopping, Checkpoint, EpochScoring"
      ],
      "metadata": {
        "id": "H3jiF5-Idg8E"
      },
      "id": "H3jiF5-Idg8E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_id_1 = '18c5DynpKSiey55WdTBkNE7Iwb7l_HL-k'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_1}', 'data2011.csv', quiet=False)\n",
        "df = pd.read_csv('data2011.csv')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2xO9N6dCt2vk"
      },
      "id": "2xO9N6dCt2vk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3NlTMLK9Zjn3",
      "metadata": {
        "id": "3NlTMLK9Zjn3"
      },
      "source": [
        "### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58751aed-cbed-44aa-92d6-565354ffb93d",
      "metadata": {
        "id": "58751aed-cbed-44aa-92d6-565354ffb93d"
      },
      "outputs": [],
      "source": [
        "## Rename Columns\n",
        "def rename(df):\n",
        "    return df.rename(columns={\n",
        "        'RREL16': 'primary_income',\n",
        "        'RREL13': 'employment_status',\n",
        "        'RREL27': 'loan_purpose',\n",
        "        'RREL25': 'original_term',\n",
        "        'RREL30': 'current_balance',\n",
        "        'RREL29': 'original_balance',\n",
        "        'RREL43': 'current_interest_rate',\n",
        "        'RREL42': 'interest_type',\n",
        "        'RREL69': 'account_status',\n",
        "        'RREL39': 'payment_due',\n",
        "        'RREL67': 'arrears_balance',\n",
        "        'RREL68': 'days_in_arrears',\n",
        "        'RREL71': 'default_amount',\n",
        "        'RREC6': 'collateral_region',\n",
        "        'RREC7': 'occupancy_type',\n",
        "        'RREC9': 'property_type',\n",
        "        'RREC16': 'original_ltv',\n",
        "        'RREC17': 'original_valuation',\n",
        "        'RREC12': 'current_ltv',\n",
        "        'RREC13': 'current_valuation',\n",
        "        'age': 'age',\n",
        "        'PrepaymentFee': 'prepayment_fee',\n",
        "        'PrepaymentHistory': 'prepayment_history',\n",
        "        'RREL30_t_1': 'past_balance',\n",
        "        'RREL39_t_1': 'past_payment_due',\n",
        "        'RREL43_t_1': 'past_interest_rate',\n",
        "        'RREC12_t_1': 'past_ltv',\n",
        "        'RREC13_t_1': 'past_valuation',\n",
        "        'incentive': 'incentive',\n",
        "        'target': 'target'\n",
        "    })\n",
        "df = rename(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f56af3df-339d-440b-b19e-1051935ca159",
      "metadata": {
        "id": "f56af3df-339d-440b-b19e-1051935ca159"
      },
      "outputs": [],
      "source": [
        "## Embed Categorical columns\n",
        "def embed(df):\n",
        "    df['employment_status'] = df['employment_status'].astype('category')\n",
        "    df['loan_purpose'] = df['loan_purpose'].astype('category')\n",
        "    df['collateral_region'] = df['collateral_region'].astype('category')\n",
        "    df['occupancy_type'] = df['occupancy_type'].astype('category')\n",
        "    df['property_type'] = df['property_type'].astype('category')\n",
        "    df['interest_type'] = df['interest_type'].astype('category')\n",
        "    df['account_status'] = df['account_status'].astype('category')\n",
        "    df['prepayment_fee'] = df['prepayment_fee'].astype('category')\n",
        "    df['prepayment_history'] = df['prepayment_history'].astype('category')\n",
        "    return df\n",
        "df = embed(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9986a0bf-2c18-4c50-9ea4-471fe6c314dd",
      "metadata": {
        "id": "9986a0bf-2c18-4c50-9ea4-471fe6c314dd"
      },
      "outputs": [],
      "source": [
        "# Define train and test sets\n",
        "X = df.drop('target', axis=1)\n",
        "Y = df['target']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QJrWGd7KfUIM",
      "metadata": {
        "id": "QJrWGd7KfUIM"
      },
      "outputs": [],
      "source": [
        "# Create one-hot encoded version\n",
        "X_train_encoded = pd.get_dummies(X_train, drop_first=True)\n",
        "X_test_encoded = pd.get_dummies(X_test, drop_first=True)\n",
        "\n",
        "# Align columns to ensure same structure\n",
        "X_test_encoded = X_test_encoded.reindex(columns=X_train_encoded.columns, fill_value=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fICK6F0RREFe",
      "metadata": {
        "id": "fICK6F0RREFe"
      },
      "outputs": [],
      "source": [
        "# Create scaled version\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_encoded)\n",
        "X_test_scaled = scaler.transform(X_test_encoded)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0dl1NevuOewv",
      "metadata": {
        "id": "0dl1NevuOewv"
      },
      "source": [
        "### CV XGBoost + GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HveAU6DKOixF",
      "metadata": {
        "id": "HveAU6DKOixF",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# --- Phase 1: XGBoost Model Training and Hyperparameter Optimization ---\n",
        "# This script implements a comprehensive machine learning pipeline for predicting mortgage prepayment,\n",
        "# utilizing XGBoost with stratified cross-validation and randomized hyperparameter search.\n",
        "# The methodology is specifically designed to address severe class imbalance.\n",
        "\n",
        "# Define the hyperparameter grid for randomized search.\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'min_child_weight': [1, 5, 10],\n",
        "    'reg_alpha': [0, 0.1, 1],\n",
        "    'reg_lambda': [0, 1, 10],\n",
        "    'scale_pos_weight': [50, 100],\n",
        "    'n_estimators': [500, 1000]\n",
        "}\n",
        "\n",
        "# Initialize the base XGBoost classifier.\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric=['aucpr', 'logloss'],\n",
        "    enable_categorical=True,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Initialize a stratified 5-fold cross-validator.\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define scoring metrics for model evaluation.\n",
        "scoring = {\n",
        "    'precision': make_scorer(precision_score, zero_division=0),\n",
        "    'recall': make_scorer(recall_score, zero_division=0),\n",
        "    'f1': make_scorer(f1_score, zero_division=0),\n",
        "    'aucpr': 'average_precision'\n",
        "}\n",
        "\n",
        "# Initialize the RandomizedSearchCV object.\n",
        "grid = RandomizedSearchCV(\n",
        "    estimator=xgb_clf,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=30,\n",
        "    scoring=scoring,\n",
        "    refit='aucpr',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Execute the randomized hyperparameter search.\n",
        "print(\"Initiating randomized hyperparameter search for XGBoost...\")\n",
        "grid.fit(X_train, y_train)\n",
        "print(\"Hyperparameter search complete.\")\n",
        "\n",
        "# Store best parameters and reinitialize final model with early stopping.\n",
        "best_params = grid.best_params_.copy()\n",
        "final_model = xgb.XGBClassifier(\n",
        "    **best_params,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric=['aucpr', 'logloss'],\n",
        "    early_stopping_rounds=50,\n",
        "    enable_categorical=True,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Train final model with early stopping.\n",
        "final_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=True)\n",
        "\n",
        "# Output best hyperparameters.\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# --- Model Interpretation: SHAP Analysis ---\n",
        "explainer = shap.TreeExplainer(final_model)\n",
        "shap_values = explainer(X_test).values\n",
        "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", max_display=20)\n",
        "\n",
        "# --- Model Evaluation ---\n",
        "y_proba = final_model.predict_proba(X_test)[:, 1]\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "# Print performance metrics.\n",
        "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"PR AUC: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "# --- Visualization ---\n",
        "# Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "PrecisionRecallDisplay.from_estimator(final_model, X_test, y_test)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Learning Curves\n",
        "results = final_model.evals_result()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(results['validation_0']['logloss'], label='Validation Log Loss')\n",
        "plt.plot(results['validation_0']['aucpr'], label='Validation AUC-PR')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('XGBoost Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0GxbcFNmZ4qC",
      "metadata": {
        "id": "0GxbcFNmZ4qC"
      },
      "source": [
        "### CV LogReg + GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7c6JuKEbZ3GY",
      "metadata": {
        "id": "7c6JuKEbZ3GY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# --- Phase 1: Logistic Regression Model Training and Hyperparameter Optimization ---\n",
        "# This script implements a traditional logistic regression baseline for predicting mortgage prepayment,\n",
        "# utilizing stratified cross-validation and randomized hyperparameter search.\n",
        "\n",
        "# Define the hyperparameter grid for randomized search.\n",
        "param_grid = {\n",
        "    'penalty': ['l1', 'l2', 'elasticnet'],\n",
        "    'C': np.logspace(-4, 4, 20),\n",
        "    'solver': ['liblinear', 'saga'],\n",
        "    'max_iter': [100, 200, 500],\n",
        "    'class_weight': [None, 'balanced', {0:1, 1:50}, {0:1, 1:100}],\n",
        "    'l1_ratio': [0, 0.25, 0.5, 0.75, 1]\n",
        "}\n",
        "\n",
        "# Initialize the base classifier.\n",
        "lr_clf = LogisticRegression(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Initialize a stratified 5-fold cross-validator.\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define scoring metrics for model evaluation.\n",
        "scoring = {\n",
        "    'precision': make_scorer(precision_score, zero_division=0),\n",
        "    'recall': make_scorer(recall_score, zero_division=0),\n",
        "    'f1': make_scorer(f1_score, zero_division=0),\n",
        "    'aucpr': 'average_precision'\n",
        "}\n",
        "\n",
        "# Initialize the RandomizedSearchCV object.\n",
        "grid = RandomizedSearchCV(\n",
        "    estimator=lr_clf,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=30,\n",
        "    scoring=scoring,\n",
        "    refit='aucpr',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Execute the randomized hyperparameter search.\n",
        "print(\"Initiating randomized hyperparameter search for Logistic Regression...\")\n",
        "grid.fit(X_train_encoded, y_train)\n",
        "print(\"Hyperparameter search complete.\")\n",
        "\n",
        "# Store the best estimator.\n",
        "best_model = grid.best_estimator_\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# --- Model Evaluation ---\n",
        "y_proba = best_model.predict_proba(X_test_encoded)[:, 1]\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "# Print performance metrics.\n",
        "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"PR AUC: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "# --- Visualization ---\n",
        "# Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "PrecisionRecallDisplay.from_estimator(best_model, X_test_encoded, y_test)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Learning Curves\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    best_model,\n",
        "    X_train_encoded,\n",
        "    y_train,\n",
        "    cv=cv,\n",
        "    scoring='average_precision',\n",
        "    n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training AUC-PR')\n",
        "plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Validation AUC-PR')\n",
        "plt.xlabel('Number of Training Examples')\n",
        "plt.ylabel('AUC-PR Score')\n",
        "plt.title('Logistic Regression Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "caLHmz_Un9KT",
      "metadata": {
        "id": "caLHmz_Un9KT"
      },
      "source": [
        "### CV RF + Grid Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rqx2e3ctoAgJ",
      "metadata": {
        "id": "rqx2e3ctoAgJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# --- Phase 1: Random Forest Model Training and Hyperparameter Optimization ---\n",
        "# This script implements a Random Forest model for predicting mortgage prepayment,\n",
        "# utilizing stratified cross-validation and randomized hyperparameter search.\n",
        "\n",
        "# Define the hyperparameter grid for randomized search.\n",
        "param_grid = {\n",
        "    'n_estimators': [100, 200, 500],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "    'max_features': ['sqrt', 'log2', None],\n",
        "    'bootstrap': [True, False],\n",
        "    'class_weight': ['balanced', 'balanced_subsample', None, {0:1, 1:50}, {0:1, 1:100}],\n",
        "    'max_samples': [None, 0.7, 0.8],\n",
        "    'ccp_alpha': [0.0, 0.01, 0.1]\n",
        "}\n",
        "\n",
        "# Initialize the base classifier.\n",
        "rf_clf = RandomForestClassifier(random_state=42, n_jobs=-1)\n",
        "\n",
        "# Initialize a stratified 5-fold cross-validator.\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define scoring metrics for model evaluation.\n",
        "scoring = {\n",
        "    'precision': make_scorer(precision_score, zero_division=0),\n",
        "    'recall': make_scorer(recall_score, zero_division=0),\n",
        "    'f1': make_scorer(f1_score, zero_division=0),\n",
        "    'aucpr': 'average_precision'\n",
        "}\n",
        "\n",
        "# Initialize the RandomizedSearchCV object.\n",
        "grid = RandomizedSearchCV(\n",
        "    estimator=rf_clf,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=30,\n",
        "    scoring=scoring,\n",
        "    refit='aucpr',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Execute the randomized hyperparameter search.\n",
        "print(\"Initiating randomized hyperparameter search for Random Forest...\")\n",
        "grid.fit(X_train_encoded, y_train)\n",
        "print(\"Hyperparameter search complete.\")\n",
        "\n",
        "# Store the best estimator.\n",
        "best_model = grid.best_estimator_\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# --- Model Evaluation ---\n",
        "y_proba = best_model.predict_proba(X_test_encoded)[:, 1]\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "# Print performance metrics.\n",
        "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"PR AUC: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "# --- Visualization ---\n",
        "# Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "PrecisionRecallDisplay.from_estimator(best_model, X_test_encoded, y_test)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Learning Curves\n",
        "train_sizes, train_scores, test_scores = learning_curve(\n",
        "    best_model,\n",
        "    X_train_encoded,\n",
        "    y_train,\n",
        "    cv=cv,\n",
        "    scoring='average_precision',\n",
        "    n_jobs=-1,\n",
        "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(train_sizes, np.mean(train_scores, axis=1), 'o-', label='Training AUC-PR')\n",
        "plt.plot(train_sizes, np.mean(test_scores, axis=1), 'o-', label='Validation AUC-PR')\n",
        "plt.xlabel('Number of Training Examples')\n",
        "plt.ylabel('AUC-PR Score')\n",
        "plt.title('Random Forest Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FBLgO3phlqRV",
      "metadata": {
        "id": "FBLgO3phlqRV"
      },
      "source": [
        "### CV NN + GridSearch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IhDbbuJdluK_",
      "metadata": {
        "id": "IhDbbuJdluK_"
      },
      "outputs": [],
      "source": [
        "# --- Phase 1: Neural Network Model Training and Hyperparameter Optimization ---\n",
        "# This script implements a Deep Neural Network (DNN) for predicting mortgage prepayment,\n",
        "# utilizing skorch for sklearn compatibility and randomized hyperparameter search.\n",
        "\n",
        "# Define the neural network architecture.\n",
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1=64, hidden_size2=30, dropout_rate=0.2):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, 1)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.bn1 = nn.BatchNorm1d(hidden_size1)\n",
        "        self.bn2 = nn.BatchNorm1d(hidden_size2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Prepare data tensors.\n",
        "X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Calculate class weight.\n",
        "pos_weight = torch.tensor([len(y_train) / y_train.sum()], dtype=torch.float32)\n",
        "\n",
        "# Define the hyperparameter grid.\n",
        "param_grid = {\n",
        "    'module__hidden_size1': [32, 64, 128],\n",
        "    'module__hidden_size2': [16, 30, 64],\n",
        "    'module__dropout_rate': [0.1, 0.2, 0.3],\n",
        "    'optimizer__lr': [0.001, 0.01, 0.0001],\n",
        "    'optimizer__weight_decay': [0, 0.001, 0.01],\n",
        "    'batch_size': [32, 64, 128],\n",
        "    'max_epochs': [50, 100, 150]\n",
        "}\n",
        "\n",
        "# Initialize callbacks.\n",
        "early_stopping = EarlyStopping(patience=10, threshold=0.001, threshold_mode='rel')\n",
        "checkpoint = Checkpoint(monitor='valid_loss_best')\n",
        "\n",
        "# Initialize the skorch classifier.\n",
        "net = NeuralNetClassifier(\n",
        "    module=NeuralNet,\n",
        "    module__input_size=X_train_tensor.shape[1],\n",
        "    criterion=nn.BCEWithLogitsLoss,\n",
        "    criterion__pos_weight=pos_weight,\n",
        "    optimizer=optim.Adam,\n",
        "    callbacks=[early_stopping, checkpoint],\n",
        "    device='cuda' if torch.cuda.is_available() else 'cpu',\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "# Initialize a stratified 5-fold cross-validator.\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Define scoring metrics for model evaluation.\n",
        "scoring = {\n",
        "    'precision': make_scorer(precision_score, zero_division=0),\n",
        "    'recall': make_scorer(recall_score, zero_division=0),\n",
        "    'f1': make_scorer(f1_score, zero_division=0),\n",
        "    'aucpr': 'average_precision'\n",
        "}\n",
        "\n",
        "# Initialize the RandomizedSearchCV object.\n",
        "nn_grid = RandomizedSearchCV(\n",
        "    estimator=net,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=20,\n",
        "    scoring=scoring,\n",
        "    refit='aucpr',\n",
        "    cv=cv,\n",
        "    n_jobs=1,\n",
        "    verbose=2,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Execute the randomized hyperparameter search.\n",
        "print(\"Initiating randomized hyperparameter search for Neural Network...\")\n",
        "nn_grid.fit(X_train_tensor, y_train_tensor)\n",
        "print(\"Hyperparameter search complete.\")\n",
        "\n",
        "# Store the best estimator.\n",
        "best_model = nn_grid.best_estimator_\n",
        "print(\"Best Parameters:\", nn_grid.best_params_)\n",
        "\n",
        "# --- Model Evaluation ---\n",
        "y_proba = best_model.predict_proba(X_test_tensor)[:, 1]\n",
        "precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "# Print performance metrics.\n",
        "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"Precision: {precision_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"Recall: {recall_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score: {f1_score(y_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"ROC AUC: {roc_auc_score(y_test, y_proba):.4f}\")\n",
        "print(f\"PR AUC: {average_precision_score(y_test, y_proba):.4f}\")\n",
        "\n",
        "# --- Visualization ---\n",
        "# Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "PrecisionRecallDisplay.from_estimator(best_model, X_test_tensor, y_test)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Learning Curves\n",
        "try:\n",
        "    history = best_model.history_\n",
        "    plt.figure(figsize=(12, 4))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(history[:, 'train_loss'], label='Train Loss')\n",
        "    plt.plot(history[:, 'valid_loss'], label='Validation Loss')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(history[:, 'valid_roc_auc'], label='Validation ROC AUC')\n",
        "    plt.plot(history[:, 'valid_average_precision'], label='Validation PR AUC')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Score')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.suptitle('Neural Network Learning Curves')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "except AttributeError:\n",
        "    print(\"Could not retrieve training history for plotting.\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "cc247b72-fb43-466d-ab4e-32dba1a8c74d",
        "3NlTMLK9Zjn3",
        "J7z2pB-VZuD4",
        "779b217c-588f-4f6a-8f92-84f88972bf8d",
        "cd9da318-ff79-49b1-9688-d8575867d92d",
        "a65abde5-2e5a-4296-b45d-be214ea9fb20",
        "8yrGbPycSHN_",
        "ac97abc9-bfb1-4f3e-8ad4-4e06dad31b42",
        "dYSlMe3xZDk6",
        "uGL2zf9RrEHW",
        "-Jh86QF3vf0E",
        "p9_zyXE7xI-v",
        "IoVqpncXjZvc",
        "V9X3ZrqrmKfw",
        "RcGQH1M0Gt21",
        "aNcHmWHlrQfg",
        "qIxcQu1etIvO",
        "j0MG7mGAHAyE",
        "SmMMIAXlHJc-",
        "wTvIpcrTH-gY",
        "Fw-MNcDYT4Qg",
        "6NKa-erwYK6f",
        "gMTszNDJcN1V",
        "XXPcnXi_m_H_",
        "-GuQRZNt1qS0",
        "PR_4Ztf2mm0-",
        "NSti2uLumqWm"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}