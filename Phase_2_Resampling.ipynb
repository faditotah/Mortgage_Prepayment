{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ErnCL4ZMZU-"
      },
      "source": [
        "#### Library & Package Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCEAa7PNMYjc"
      },
      "outputs": [],
      "source": [
        "!pip install skorch torch scikit-learn\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics import (\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    ConfusionMatrixDisplay,\n",
        "    roc_auc_score,\n",
        "    roc_curve,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    precision_recall_curve,\n",
        "    accuracy_score,\n",
        "    log_loss,\n",
        "    PrecisionRecallDisplay,\n",
        "    make_scorer,\n",
        "    RocCurveDisplay\n",
        ")\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GridSearchCV, RepeatedStratifiedKFold, RandomizedSearchCV\n",
        "from sklearn.metrics import make_scorer, average_precision_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "import seaborn as sns\n",
        "import imblearn\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from xgboost import cv\n",
        "from xgboost import XGBClassifier\n",
        "import scipy as stats\n",
        "from scipy.spatial.distance import mahalanobis\n",
        "from numpy.linalg import inv\n",
        "from scipy.spatial import distance\n",
        "from skorch import NeuralNetClassifier\n",
        "from skorch.callbacks import EarlyStopping, Checkpoint, EpochScoring\n",
        "from skorch.helper import predefined_split\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1XjFadbPw30m",
        "outputId": "7cd34139-dd5b-4e11-9357-4f058048a3b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.4-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from optuna) (25.0)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from optuna) (2.0.43)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.12/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.12/dist-packages (from alembic>=1.5.0->optuna) (4.14.1)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.4)\n",
            "Downloading optuna-4.5.0-py3-none-any.whl (400 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m400.9/400.9 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.4-py3-none-any.whl (247 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.4 colorlog-6.9.0 optuna-4.5.0\n"
          ]
        }
      ],
      "source": [
        "# Import optuna for faster processing\n",
        "!pip install optuna\n",
        "import optuna"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00gGl21bI0zu"
      },
      "source": [
        "#### Import Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mCey7DavMgKM"
      },
      "outputs": [],
      "source": [
        "file_id_1 = '18c5DynpKSiey55WdTBkNE7Iwb7l_HL-k'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_1}', 'data2011.csv', quiet=False)\n",
        "df1 = pd.read_csv('data2011.csv')\n",
        "\n",
        "file_id_2 = '1bJsC9bUmrMHXlKIv82Gkl-Qxldy9D-KQ'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_2}', 'data2102.csv', quiet=False)\n",
        "df2 = pd.read_csv('data2102.csv')\n",
        "\n",
        "file_id_3 = '1BU41bihK6rCTVWmyUFr4gEmYwIclKeMD'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_3}', 'data2105.csv', quiet=False)\n",
        "df3 = pd.read_csv('data2105.csv')\n",
        "\n",
        "file_id_4 = '1VUA3AgnL7ouqCY3vrui7G6qr5RbbJwDQ'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_4}', 'data2108.csv', quiet=False)\n",
        "df4 = pd.read_csv('data2108.csv')\n",
        "\n",
        "file_id_5 = '1GSL8AOlv9fWylFU-HAKbIbOCxuN1b754'\n",
        "gdown.download(f'https://drive.google.com/uc?id={file_id_5}', 'data2111.csv', quiet=False)\n",
        "df5 = pd.read_csv('data2111.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TEFCNdYAI4CY"
      },
      "source": [
        "#### Data Processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cT6U2R4IM1ri"
      },
      "outputs": [],
      "source": [
        "## Rename Columns\n",
        "def rename(df):\n",
        "    return df.rename(columns={\n",
        "        'RREL16': 'primary_income',\n",
        "        'RREL13': 'employment_status',\n",
        "        'RREL27': 'loan_purpose',\n",
        "        'RREL25': 'original_term',\n",
        "        'RREL30': 'current_balance',\n",
        "        'RREL29': 'original_balance',\n",
        "        'RREL43': 'current_interest_rate',\n",
        "        'RREL42': 'interest_type',\n",
        "        'RREL69': 'account_status',\n",
        "        'RREL39': 'payment_due',\n",
        "        'RREL67': 'arrears_balance',\n",
        "        'RREL68': 'days_in_arrears',\n",
        "        'RREL71': 'default_amount',\n",
        "        'RREC6': 'collateral_region',\n",
        "        'RREC7': 'occupancy_type',\n",
        "        'RREC9': 'property_type',\n",
        "        'RREC16': 'original_ltv',\n",
        "        'RREC17': 'original_valuation',\n",
        "        'RREC12': 'current_ltv',\n",
        "        'RREC13': 'current_valuation',\n",
        "        'age': 'age',\n",
        "        'PrepaymentFee': 'prepayment_fee',\n",
        "        'PrepaymentHistory': 'prepayment_history',\n",
        "        'RREL30_t_1': 'past_balance',\n",
        "        'RREL39_t_1': 'past_payment_due',\n",
        "        'RREL43_t_1': 'past_interest_rate',\n",
        "        'RREC12_t_1': 'past_ltv',\n",
        "        'RREC13_t_1': 'past_valuation',\n",
        "        'incentive': 'incentive',\n",
        "        'target': 'target'\n",
        "    })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7K_c3xeM8e-"
      },
      "outputs": [],
      "source": [
        "## Embed Categorical columns\n",
        "def embed(df):\n",
        "    df['employment_status'] = df['employment_status'].astype('category')\n",
        "    df['loan_purpose'] = df['loan_purpose'].astype('category')\n",
        "    df['collateral_region'] = df['collateral_region'].astype('category')\n",
        "    df['occupancy_type'] = df['occupancy_type'].astype('category')\n",
        "    df['property_type'] = df['property_type'].astype('category')\n",
        "    df['interest_type'] = df['interest_type'].astype('category')\n",
        "    df['account_status'] = df['account_status'].astype('category')\n",
        "    df['prepayment_fee'] = df['prepayment_fee'].astype('category')\n",
        "    df['prepayment_history'] = df['prepayment_history'].astype('category')\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BSpwL4iNNEo"
      },
      "outputs": [],
      "source": [
        "# Rename variables\n",
        "df1 = rename(df1)\n",
        "df2 = rename(df2)\n",
        "df3 = rename(df3)\n",
        "df4 = rename(df4)\n",
        "df5 = rename(df5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eNIQovr2XzpG"
      },
      "outputs": [],
      "source": [
        "# Drop single PNNR observation\n",
        "df5 = df5[df5['employment_status'] != 'PNNR']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB-HQU2VyJI0"
      },
      "outputs": [],
      "source": [
        "# Embed categorical variables\n",
        "df1 = embed(df1)\n",
        "df2 = embed(df2)\n",
        "df3 = embed(df3)\n",
        "df4 = embed(df4)\n",
        "df5 = embed(df5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xzMBYKj4NNqb"
      },
      "outputs": [],
      "source": [
        "# Split data from targets\n",
        "X1 = df1.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y1 = df1['target']\n",
        "X2 = df2.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y2 = df2['target']\n",
        "X3 = df3.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y3 = df3['target']\n",
        "X4 = df4.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y4 = df4['target']\n",
        "X5 = df5.drop(['target', 'prepayment_fee'], axis=1)\n",
        "y5 = df5['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jkq-2aulyxOx"
      },
      "source": [
        "#### Mahalanobis Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CDBAmxzHZLIf"
      },
      "outputs": [],
      "source": [
        "# Concatenate the datasets\n",
        "temp = pd.concat([df1, df2], ignore_index=True)\n",
        "temp = temp.drop(['prepayment_fee'], axis=1)\n",
        "\n",
        "# Store categorical columns before dropping them\n",
        "categorical_cols = ['employment_status', 'loan_purpose', 'collateral_region',\n",
        "                   'occupancy_type', 'property_type', 'interest_type',\n",
        "                   'account_status', 'prepayment_history']\n",
        "\n",
        "# Create a DataFrame with just the categorical variables and index\n",
        "categorical_data = temp[categorical_cols].copy()\n",
        "categorical_data['original_index'] = temp.index\n",
        "\n",
        "# Separate numeric features and target (drop categoricals)\n",
        "numeric_temp = temp.drop(categorical_cols, axis=1)\n",
        "\n",
        "# Separate features and target\n",
        "X_temp = numeric_temp.drop('target', axis=1)\n",
        "y_temp = numeric_temp['target']\n",
        "\n",
        "# Scale numeric features only\n",
        "scaler = StandardScaler()\n",
        "X_scaled = pd.DataFrame(scaler.fit_transform(X_temp), columns=X_temp.columns)\n",
        "\n",
        "# Reattach target to scaled numeric features\n",
        "temp_scaled = pd.concat([X_scaled, y_temp], axis=1)\n",
        "\n",
        "# Split into prepaid and non-prepaid\n",
        "prepaid_df = temp_scaled.loc[temp_scaled['target'] == 1]\n",
        "non_prepaid_df = temp_scaled.loc[temp_scaled['target'] == 0]\n",
        "\n",
        "# Extract just the numeric features (drop target)\n",
        "X_prepaid = prepaid_df.drop(columns=['target']).values\n",
        "X_non_prepaid = non_prepaid_df.drop(columns=['target']).values\n",
        "\n",
        "# Compute the mean and covariance of the prepaid group\n",
        "mean_vec = np.mean(X_prepaid, axis=0)\n",
        "cov_matrix = np.cov(X_prepaid, rowvar=False)\n",
        "inv_cov_matrix = inv(cov_matrix + np.eye(cov_matrix.shape[0]) * 1e-6)  # regularization\n",
        "\n",
        "# Compute Mahalanobis distances for each non-prepaid observation\n",
        "mahal_distances = [distance.mahalanobis(x, mean_vec, inv_cov_matrix) for x in X_non_prepaid]\n",
        "\n",
        "# Add distances to non_prepaid_df\n",
        "non_prepaid_df = non_prepaid_df.copy()\n",
        "non_prepaid_df['mahal_dist'] = mahal_distances\n",
        "\n",
        "# Sort by distance and select the closest N non-prepaid samples (e.g. 2x the number of prepaids)\n",
        "n = len(prepaid_df) * 2\n",
        "selected_non_prepaids = non_prepaid_df.nsmallest(n, 'mahal_dist').drop(columns=['mahal_dist'])\n",
        "\n",
        "# Combine with prepaid\n",
        "balanced_numeric_df = pd.concat([prepaid_df, selected_non_prepaids], ignore_index=True)\n",
        "\n",
        "# Get the original indices of the selected samples\n",
        "selected_indices = balanced_numeric_df.index\n",
        "\n",
        "# Retrieve the corresponding categorical data\n",
        "selected_categorical_data = categorical_data.loc[categorical_data['original_index'].isin(selected_indices)]\n",
        "\n",
        "# Drop the original_index column we added\n",
        "selected_categorical_data = selected_categorical_data.drop(columns=['original_index'])\n",
        "\n",
        "# Reset indices for proper merging\n",
        "balanced_numeric_df = balanced_numeric_df.reset_index(drop=True)\n",
        "selected_categorical_data = selected_categorical_data.reset_index(drop=True)\n",
        "\n",
        "# Combine numeric and categorical data\n",
        "final_balanced_df = pd.concat([balanced_numeric_df, selected_categorical_data], axis=1)\n",
        "\n",
        "# Concatenate test data\n",
        "temp = pd.concat([df3, df4], ignore_index=True)\n",
        "temp = temp.drop(['prepayment_fee'], axis=1)\n",
        "\n",
        "# Get categorical data from test data\n",
        "categorical_data = temp[categorical_cols].copy()\n",
        "categorical_data['original_index'] = temp.index\n",
        "\n",
        "# Get numeric data from test data\n",
        "numeric_temp = temp.drop(categorical_cols, axis=1)\n",
        "X_temp = numeric_temp.drop('target', axis=1)\n",
        "y_temp = numeric_temp['target']\n",
        "\n",
        "# Scale numeric test data\n",
        "X_scaled = pd.DataFrame(scaler.transform(X_temp), columns=X_temp.columns)\n",
        "\n",
        "# Reset index for merge of categorical and scaled numeric test data\n",
        "X_scaled = X_scaled.reset_index(drop=True)\n",
        "selected_categorical_data = categorical_data.loc[categorical_data['original_index'].isin(X_temp.index)].drop(columns=['original_index']).reset_index(drop=True)\n",
        "\n",
        "# Final validation set\n",
        "X_mah_val = pd.concat([X_scaled, selected_categorical_data], axis=1)\n",
        "y_mah_val = pd.concat([y3, y4], ignore_index=True)\n",
        "\n",
        "# Final train set\n",
        "X_mah_train = final_balanced_df.drop(['target'], axis=1)\n",
        "y_mah_train = final_balanced_df['target']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGhfFRGDPIeV"
      },
      "source": [
        "#### R-GAN Oversampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YYJJHPePylmm"
      },
      "outputs": [],
      "source": [
        "# Function to keep numeric variables\n",
        "def keep(datasets):\n",
        "    columns_to_keep = ['primary_income', 'original_term', 'current_balance', 'original_balance', 'current_interest_rate',\n",
        "'payment_due', 'arrears_balance', 'days_in_arrears', 'default_amount', 'original_ltv', 'current_ltv',\n",
        "'original_valuation', 'current_valuation', 'age', 'past_balance', 'past_payment_due', 'past_interest_rate',\n",
        "'past_ltv', 'past_valuation', 'incentive', 'target']\n",
        "    return [df[columns_to_keep] for df in datasets]\n",
        "\n",
        "X1_gan, X2_gan = keep([df1, df2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tb-f0iTK2WPy"
      },
      "outputs": [],
      "source": [
        "# Concatenate training data\n",
        "X_gan = pd.concat([X1_gan, X2_gan], ignore_index = True)\n",
        "# Extract positive observations\n",
        "X_gan_pos = X_gan.loc[X_gan['target'] == 1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qqYpxcp33jzD"
      },
      "outputs": [],
      "source": [
        "# Remove targets and scale numeric variables\n",
        "X_gan_pos = X_gan_pos.drop(columns=[\"target\"])\n",
        "minority_data = X_gan_pos.to_numpy().astype(np.float32)\n",
        "scaler = StandardScaler()\n",
        "minority_data = scaler.fit_transform(minority_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DfNt2hppPMGn"
      },
      "outputs": [],
      "source": [
        "# Define the Generator network: creates synthetic data from random noise\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, input_dim=8, output_dim=30):\n",
        "        super().__init__()\n",
        "        # Build the generator as a sequence of layers\n",
        "        self.model = nn.Sequential(\n",
        "            nn.utils.spectral_norm(nn.Linear(input_dim, 32)),  # Spectral norm helps training stability\n",
        "            nn.LeakyReLU(0.2),                                # LeakyReLU avoids dead neurons\n",
        "            nn.utils.spectral_norm(nn.Linear(32, 128)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.spectral_norm(nn.Linear(128, 512)),\n",
        "            nn.LeakyReLU(0.2),\n",
        "            nn.utils.spectral_norm(nn.Linear(512, output_dim)), # Output layer: generates synthetic features\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        # Pass random noise through the network to generate data\n",
        "        return self.model(z)\n",
        "\n",
        "# Define the Discriminator network: distinguishes real from synthetic data\n",
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, input_dim=30):\n",
        "        super().__init__()\n",
        "        # Build discriminator with separate layer sequences\n",
        "        self.layer1 = nn.Sequential(\n",
        "            nn.Linear(input_dim, 512),\n",
        "            nn.LayerNorm(512),  # Layer normalization stabilizes training\n",
        "            nn.CELU()           # CELU activation: smooth alternative to ReLU\n",
        "        )\n",
        "        self.layer2 = nn.Sequential(\n",
        "            nn.Linear(512, 128),\n",
        "            nn.LayerNorm(128),\n",
        "            nn.CELU()\n",
        "        )\n",
        "        self.layer3 = nn.Sequential(\n",
        "            nn.Linear(128, 32),\n",
        "            nn.LayerNorm(32),\n",
        "            nn.CELU()\n",
        "        )\n",
        "        self.layer4 = nn.Sequential(\n",
        "            nn.Linear(32, 8),\n",
        "            nn.LayerNorm(8),\n",
        "            nn.CELU()\n",
        "        )\n",
        "        self.output = nn.Linear(8, 1)  # Final output: real/fake score\n",
        "\n",
        "    def forward(self, x, return_hidden=False):\n",
        "        # Pass input through all layers\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        # Optionally return hidden layer features for additional loss calculation\n",
        "        if return_hidden:\n",
        "            return self.output(x), x\n",
        "        return self.output(x)\n",
        "\n",
        "# Custom loss function to ensure synthetic data matches real data statistics\n",
        "def similarity_loss(real_hidden, fake_hidden):\n",
        "    # Calculate mean feature values for real and synthetic data\n",
        "    real_mean = torch.mean(real_hidden, dim=0)\n",
        "    fake_mean = torch.mean(fake_hidden, dim=0)\n",
        "    # Use mean squared error to make distributions similar\n",
        "    return F.mse_loss(real_mean, fake_mean)\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "# Training configuration parameters\n",
        "z_dim = 8              # Size of random noise vector input to generator\n",
        "feature_dim = 20       # Number of features in output data\n",
        "batch_size = 64        # Number of samples per training batch\n",
        "epochs = 1000          # Total number of training iterations\n",
        "lr = 0.0001            # Learning rate for discriminator\n",
        "lambda_gp = 10         # Weight for gradient penalty loss\n",
        "eta_sim = 0.01         # Weight for similarity loss\n",
        "\n",
        "# Set up device (GPU if available, else CPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Initialize generator and discriminator networks\n",
        "G = Generator(input_dim=z_dim, output_dim=feature_dim).to(device)\n",
        "D = Discriminator(input_dim=feature_dim).to(device)\n",
        "\n",
        "# Set up optimizers for both networks\n",
        "g_opt = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.9))  # Generator optimizer\n",
        "d_opt = optim.Adam(D.parameters(), lr=lr, betas=(0.5, 0.9))      # Discriminator optimizer\n",
        "\n",
        "# Prepare real minority class data for training\n",
        "real_dataset = TensorDataset(torch.tensor(minority_data, dtype=torch.float32))\n",
        "real_loader = DataLoader(real_dataset, batch_size=batch_size, shuffle=True)  # Data loader for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f7pBC9ZI4rx4"
      },
      "outputs": [],
      "source": [
        "# Calculates the gradient penalty for WGAN-GP, which helps stabilize training\n",
        "def gradient_penalty(D, real_data, fake_data):\n",
        "    # Create random interpolation between real and fake data\n",
        "    alpha = torch.rand(real_data.size(0), 1).to(device)\n",
        "    alpha = alpha.expand_as(real_data)\n",
        "    interpolates = (alpha * real_data + ((1 - alpha) * fake_data)).requires_grad_(True)\n",
        "\n",
        "    # Get discriminator's opinion on the interpolated data\n",
        "    d_interpolates = D(interpolates)\n",
        "\n",
        "    # Calculate gradients of the discriminator's output with respect to the interpolated data\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=d_interpolates, inputs=interpolates,\n",
        "        grad_outputs=torch.ones_like(d_interpolates),\n",
        "        create_graph=True, retain_graph=True\n",
        "    )[0]\n",
        "\n",
        "    # Calculate the norm (magnitude) of these gradients\n",
        "    grad_norm = gradients.norm(2, dim=1)\n",
        "\n",
        "    # Penalize gradients that deviate from 1 (Lipschitz constraint)\n",
        "    return lambda_gp * ((grad_norm - 1) ** 2).mean()\n",
        "\n",
        "# Main training loop\n",
        "for epoch in range(epochs):\n",
        "    # Process data in batches\n",
        "    for real_batch, in real_loader:\n",
        "        real_batch = real_batch.to(device)\n",
        "\n",
        "        # === Train Discriminator ===\n",
        "        # Multiple steps to ensure the discriminator is well-trained before generator updates\n",
        "        for _ in range(4):  # Discriminator steps\n",
        "            # Generate fake data from random noise\n",
        "            z = torch.randn(real_batch.size(0), z_dim).to(device)\n",
        "            fake_data = G(z).detach()  # Detach to avoid training generator here\n",
        "\n",
        "            # Get discriminator scores for real and fake data\n",
        "            d_real = D(real_batch)\n",
        "            d_fake = D(fake_data)\n",
        "\n",
        "            # Calculate gradient penalty for stability\n",
        "            gp = gradient_penalty(D, real_batch, fake_data)\n",
        "\n",
        "            # Wasserstein loss with gradient penalty\n",
        "            d_loss = -torch.mean(d_real) + torch.mean(d_fake) + gp\n",
        "\n",
        "            # Update discriminator weights\n",
        "            D.zero_grad()\n",
        "            d_loss.backward()\n",
        "            d_opt.step()\n",
        "\n",
        "        # === Train Generator ===\n",
        "        # Multiple steps to improve generator against current discriminator\n",
        "        for _ in range(4):  # Generator steps\n",
        "            # Generate new fake data\n",
        "            z = torch.randn(real_batch.size(0), z_dim).to(device)\n",
        "            fake_data = G(z)\n",
        "\n",
        "            # Get discriminator scores and hidden features for both real and fake data\n",
        "            d_fake_score, fake_hidden = D(fake_data, return_hidden=True)\n",
        "            _, real_hidden = D(real_batch, return_hidden=True)\n",
        "\n",
        "            # Calculate how similar the distributions of real and fake features are\n",
        "            sim_loss = similarity_loss(real_hidden, fake_hidden)\n",
        "\n",
        "            # Total generator loss: try to fool discriminator + match feature distributions\n",
        "            g_loss = -torch.mean(d_fake_score) + eta_sim * sim_loss\n",
        "\n",
        "            # Update generator weights\n",
        "            G.zero_grad()\n",
        "            g_loss.backward()\n",
        "            g_opt.step()\n",
        "\n",
        "    # Print progress every 100 epochs\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"[Epoch {epoch}] D_loss: {d_loss.item():.4f}, G_loss: {g_loss.item():.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bk5UZHFV48Q2"
      },
      "outputs": [],
      "source": [
        "# Create function for synthetic data generation\n",
        "def generate_synthetic(G, n_samples=1000, z_dim=8):\n",
        "    G.eval()\n",
        "    with torch.no_grad():\n",
        "        z = torch.randn(n_samples, z_dim).to(device)\n",
        "        synth_data = G(z).cpu().numpy()\n",
        "    return synth_data\n",
        "\n",
        "# Create synthetic data\n",
        "synthetic_minority = generate_synthetic(G, n_samples=10000)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbfZ5x8S6QTC"
      },
      "outputs": [],
      "source": [
        "columns_to_keep = ['primary_income', 'original_term', 'current_balance', 'original_balance', 'current_interest_rate',\n",
        "'payment_due', 'arrears_balance', 'days_in_arrears', 'default_amount', 'original_ltv', 'current_ltv',\n",
        "'original_valuation', 'current_valuation', 'age', 'past_balance', 'past_payment_due', 'past_interest_rate',\n",
        "'past_ltv', 'past_valuation', 'incentive']\n",
        "\n",
        "# Use synthetic_minority directly, because it is already scaled\n",
        "synth_df = pd.DataFrame(synthetic_minority, columns=columns_to_keep)\n",
        "synth_df['target'] = 1\n",
        "\n",
        "# Use X_gan also in its scaled version:\n",
        "X_gan_scaled = X_gan.copy()\n",
        "X_gan_scaled[columns_to_keep] = scaler.transform(X_gan[columns_to_keep])\n",
        "\n",
        "augmented_df = pd.concat([X_gan_scaled, synth_df], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QsVZi7it9wP-"
      },
      "outputs": [],
      "source": [
        "# Split targets from data\n",
        "X_gan_train = augmented_df.drop(['target'], axis=1)\n",
        "y_gan_train = augmented_df['target']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SPkv_NHr-gsr"
      },
      "outputs": [],
      "source": [
        "# Keep numeric columns\n",
        "X3_gan, X4_gan = keep([df3, df4])\n",
        "gan_test = pd.concat([X3_gan, X4_gan], ignore_index=True)\n",
        "\n",
        "# Split and scale, then convert back to DataFrame\n",
        "X_gan_test_raw = gan_test.drop(columns=[\"target\"])\n",
        "X_gan_test = pd.DataFrame(\n",
        "    scaler.transform(X_gan_test_raw),\n",
        "    columns=X_gan_test_raw.columns,\n",
        "    index=X_gan_test_raw.index\n",
        ")\n",
        "# Target\n",
        "y_gan_test = gan_test[\"target\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGJJJ0wG-ZcF"
      },
      "source": [
        "## R-GAN Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'min_child_weight': [1, 5, 10],\n",
        "    'reg_alpha': [0, 0.1, 1],\n",
        "    'reg_lambda': [0, 1, 10],\n",
        "    'scale_pos_weight': [50, 100],\n",
        "    'n_estimators': [500, 1000]\n",
        "}\n",
        "\n",
        "# Set up XGBoost without early stopping in the initializer\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric=['aucpr', 'logloss'],\n",
        "    enable_categorical=True,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 5-Fold Stratified Cross Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Custom scoring for imbalanced data\n",
        "scoring = {\n",
        "    'precision': make_scorer(precision_score, zero_division=0),\n",
        "    'recall': make_scorer(recall_score, zero_division=0),\n",
        "    'f1': make_scorer(f1_score, zero_division=0),\n",
        "    'aucpr': 'average_precision'\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "grid = RandomizedSearchCV(\n",
        "    estimator=xgb_clf,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=30,  # Reduced for faster execution\n",
        "    scoring=scoring,\n",
        "    refit='aucpr',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit without early stopping in grid search\n",
        "grid.fit(X_gan_train, y_gan_train)\n",
        "\n",
        "# Now train final model with early stopping using best params\n",
        "best_params = grid.best_params_.copy()\n",
        "\n",
        "\n",
        "final_model = xgb.XGBClassifier(\n",
        "    **best_params,\n",
        "    objective='binary:logistic',\n",
        "    eval_metric=['aucpr', 'logloss'],\n",
        "    early_stopping_rounds=50,\n",
        "    enable_categorical=True,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Now apply early stopping\n",
        "final_model.fit(\n",
        "    X_gan_train, y_gan_train,\n",
        "    eval_set=[(X_gan_test, y_gan_test)],\n",
        "    verbose=True\n",
        ")\n",
        "# Best model\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate on test set with optimal threshold\n",
        "y_proba = final_model.predict_proba(X_gan_test)[:, 1]\n",
        "\n",
        "# Find optimal threshold\n",
        "precision, recall, thresholds = precision_recall_curve(y_gan_test, y_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "# Metrics\n",
        "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_gan_test, y_pred)}\")\n",
        "print(f\"Precision: {precision_score(y_gan_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_gan_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_gan_test, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"ROC AUC:   {roc_auc_score(y_gan_test, y_proba):.4f}\")\n",
        "print(f\"PR AUC:    {average_precision_score(y_gan_test, y_proba):.4f}\")\n",
        "\n",
        "# Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = PrecisionRecallDisplay.from_estimator(final_model, X_gan_test, y_gan_test)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Learning curves\n",
        "results = final_model.evals_result()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(results['validation_0']['logloss'], label='Test Log Loss')\n",
        "plt.plot(results['validation_0']['aucpr'], label='Test AUC-PR')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# 1. ROC Curve (Enhanced)\n",
        "plt.figure(figsize=(8, 6))\n",
        "roc_auc = roc_auc_score(y_gan_test, y_proba)\n",
        "\n",
        "# Plot without automatic legend\n",
        "RocCurveDisplay.from_estimator(\n",
        "    final_model,\n",
        "    X_gan_test,\n",
        "    y_gan_test,\n",
        "    name=None  # Disable auto-legend\n",
        ")\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.5)')\n",
        "plt.title('Oversampled ROC Curve', fontsize=12, pad=20)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add custom legend\n",
        "plt.legend(\n",
        "    [f\"XGBoost (AUC = {roc_auc:.2f})\", \"Random Classifier\"],\n",
        "    loc='lower right',\n",
        "    framealpha=1\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# 2. Precision-Recall Curve (Enhanced)\n",
        "plt.figure(figsize=(8, 6))\n",
        "ap_score = average_precision_score(y_gan_test, y_proba)\n",
        "\n",
        "# Create without auto-legend\n",
        "PrecisionRecallDisplay.from_predictions(\n",
        "    y_gan_test,\n",
        "    y_proba,\n",
        "    name=\"XGBoost\"\n",
        ")\n",
        "plt.title('Oversampled Precision-Recall Curve', fontsize=12, pad=20)\n",
        "plt.grid(True, alpha=0.3)\n",
        "\n",
        "# Add custom legend\n",
        "plt.legend(\n",
        "    [f\"XGBoost (AP = {ap_score:.2f})\"],\n",
        "    loc='upper right',\n",
        "    framealpha=1\n",
        ")\n",
        "plt.show()\n",
        "\n",
        "# 3. Enhanced Learning Curves\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.plot(results['validation_0']['logloss'],\n",
        "         label='Validation Log Loss',\n",
        "         color='#1f77b4',\n",
        "         linewidth=2)\n",
        "plt.plot(results['validation_0']['aucpr'],\n",
        "         label='Validation PR-AUC',\n",
        "         color='#ff7f0e',\n",
        "         linewidth=2)\n",
        "plt.xlabel('Boosting Rounds', fontsize=10)\n",
        "plt.ylabel('Metric Value', fontsize=10)\n",
        "plt.title('(c) XGBoost Learning Curves', fontsize=12, pad=20)\n",
        "plt.legend(fontsize=10)\n",
        "plt.grid(True, alpha=0.3)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "b1fFTb4h0bCe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVOq2k0DBazw"
      },
      "source": [
        "## Mahalanobis Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define hyperparameter grid\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'colsample_bytree': [0.7, 0.8, 0.9],\n",
        "    'gamma': [0, 0.1, 0.2],\n",
        "    'min_child_weight': [1, 5, 10],\n",
        "    'reg_alpha': [0, 0.1, 1],\n",
        "    'reg_lambda': [0, 1, 10],\n",
        "    'scale_pos_weight': [50, 100],\n",
        "    'n_estimators': [500, 1000]\n",
        "}\n",
        "\n",
        "# Set up XGBoost without early stopping in the initializer\n",
        "xgb_clf = xgb.XGBClassifier(\n",
        "    objective='binary:logistic',\n",
        "    eval_metric=['aucpr', 'logloss'],\n",
        "    enable_categorical=True,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=0,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# 5-Fold Stratified Cross Validation\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# Custom scoring for imbalanced data\n",
        "scoring = {\n",
        "    'precision': make_scorer(precision_score, zero_division=0),\n",
        "    'recall': make_scorer(recall_score, zero_division=0),\n",
        "    'f1': make_scorer(f1_score, zero_division=0),\n",
        "    'aucpr': 'average_precision'\n",
        "}\n",
        "\n",
        "# RandomizedSearchCV\n",
        "grid = RandomizedSearchCV(\n",
        "    estimator=xgb_clf,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=30,  # Reduced for faster execution\n",
        "    scoring=scoring,\n",
        "    refit='aucpr',\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "# Fit without early stopping in grid search\n",
        "grid.fit(X_mah_train, y_mah_train)\n",
        "\n",
        "# Now train final model with early stopping using best params\n",
        "best_params = grid.best_params_.copy()\n",
        "\n",
        "\n",
        "final_model = xgb.XGBClassifier(\n",
        "    **best_params,  # Includes all best params except n_estimators\n",
        "    objective='binary:logistic',\n",
        "    eval_metric=['aucpr', 'logloss'],\n",
        "    early_stopping_rounds=50,\n",
        "    enable_categorical=True,\n",
        "    use_label_encoder=False,\n",
        "    verbosity=1,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Now apply early stopping\n",
        "final_model.fit(\n",
        "    X_mah_train, y_mah_train,\n",
        "    eval_set=[(X_mah_val, y_mah_val)],\n",
        "    verbose=True\n",
        ")\n",
        "# Best model\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "\n",
        "# Evaluate on test set with optimal threshold\n",
        "y_proba = final_model.predict_proba(X_mah_val)[:, 1]\n",
        "\n",
        "# Find optimal threshold\n",
        "precision, recall, thresholds = precision_recall_curve(y_mah_val, y_proba)\n",
        "f1_scores = 2 * (precision * recall) / (precision + recall + 1e-9)\n",
        "optimal_idx = np.argmax(f1_scores)\n",
        "optimal_threshold = thresholds[optimal_idx]\n",
        "y_pred = (y_proba >= optimal_threshold).astype(int)\n",
        "\n",
        "# Metrics\n",
        "print(f\"\\nOptimal Threshold: {optimal_threshold:.4f}\")\n",
        "print(f\"Confusion Matrix:\\n{confusion_matrix(y_mah_val, y_pred)}\")\n",
        "print(f\"Precision: {precision_score(y_mah_val, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"Recall:    {recall_score(y_mah_val, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"F1 Score:  {f1_score(y_mah_val, y_pred, zero_division=0):.4f}\")\n",
        "print(f\"ROC AUC:   {roc_auc_score(y_mah_val, y_proba):.4f}\")\n",
        "print(f\"PR AUC:    {average_precision_score(y_mah_val, y_proba):.4f}\")\n",
        "\n",
        "# Precision-Recall Curve\n",
        "plt.figure(figsize=(8, 6))\n",
        "disp = PrecisionRecallDisplay.from_estimator(final_model, X_mah_val, y_mah_val)\n",
        "plt.title('Precision-Recall Curve')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Learning curves\n",
        "results = final_model.evals_result()\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(results['validation_0']['logloss'], label='Test Log Loss')\n",
        "plt.plot(results['validation_0']['aucpr'], label='Test AUC-PR')\n",
        "plt.xlabel('Iterations')\n",
        "plt.ylabel('Metric Value')\n",
        "plt.title('Learning Curves')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5kYkG4_Z16US"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "1ErnCL4ZMZU-",
        "00gGl21bI0zu",
        "TEFCNdYAI4CY",
        "byT3Z69MI9kt",
        "0i6-1aeNyV0h",
        "csSMBzN0yknJ",
        "ax1bgDWTynzk",
        "Jkq-2aulyxOx",
        "RGhfFRGDPIeV",
        "cWknZPH6apwL",
        "bVOq2k0DBazw",
        "UvdMmMGAyt_S"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}